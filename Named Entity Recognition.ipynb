{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a08311fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\nicolas.deza\\appdata\\local\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "WARNING:tensorflow:From C:\\Users\\nicolas.deza\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from contextlib import redirect_stdout\n",
    "!pip install tensorflow\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, concatenate, Lambda\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "from os import listdir\n",
    "from xml.dom.minidom import parse\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b542304",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8fa8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Reads all XML files in a given directory and generates a list of sentences.\n",
    "    Each sentence is a list of tuples (word, start, end, tag)\n",
    "    \"\"\"\n",
    "    def __init__(self, datadir):\n",
    "        self.data = {}\n",
    "        # Process each file in the directory\n",
    "        for f in listdir(datadir):\n",
    "\n",
    "            # Parse the XML file, get a DOM tree\n",
    "            tree = parse(datadir + \"/\" + f)\n",
    "\n",
    "            # Process each sentence in the file\n",
    "            sentences = tree.getElementsByTagName(\"sentence\")\n",
    "            for s in sentences:\n",
    "                sid = s.attributes[\"id\"].value  # Get the sentence ID\n",
    "                stext = s.attributes[\"text\"].value  # Get the sentence text\n",
    "                entities = s.getElementsByTagName(\"entity\")\n",
    "\n",
    "                spans = []\n",
    "                for e in entities:\n",
    "                    # For discontinuous entities, we only get the first span.\n",
    "                    # Obviously, the model will not work for this type of entities. Their encoding is complex.\n",
    "                    # But it doesn't matter much, there are very few.\n",
    "                    (start, end) = e.attributes[\"charOffset\"].value.split(\";\")[0].split(\"-\")\n",
    "                    typ = e.attributes[\"type\"].value\n",
    "                    spans.append((int(start), int(end), typ))\n",
    "\n",
    "                # Convert the sentence into a sequence of tokens\n",
    "                tokens = self.__tokenize(stext)\n",
    "\n",
    "                # Add the Gold Standard tag\n",
    "                self.data[sid] = []\n",
    "                for i in range(0, len(tokens)):\n",
    "                    # Check if the token is part of the tag\n",
    "                    tokens[i]['tag'] = self.__get_tag(tokens[i], spans)\n",
    "                    self.data[sid].append(tokens[i])\n",
    "\n",
    "    ## --------- Tokenize the sentence -----------\n",
    "    ## Tokenizes the sentence\n",
    "    def __tokenize(self, txt):\n",
    "        offset = 0\n",
    "        tks = []\n",
    "        ## word_tokenize separates the words\n",
    "        for t in word_tokenize(txt):\n",
    "            ## Manage the position (using the offset) where each token should appear\n",
    "            offset = txt.find(t, offset)\n",
    "            tks.append({'lc_form': t.lower(), 'form': t, 'start': offset, 'end': offset + len(t) - 1})\n",
    "            offset += len(t)\n",
    "\n",
    "        ## List of tuples\n",
    "        return tks\n",
    "\n",
    "    ## --------- Get the Tag -----------\n",
    "    def __get_tag(self, token, spans):\n",
    "        for (spanS, spanE, spanT) in spans:\n",
    "            if token['start'] == spanS and token['end'] <= spanE:\n",
    "                return \"B-\" + spanT\n",
    "            elif token['start'] >= spanS and token['end'] <= spanE:\n",
    "                return \"I-\" + spanT\n",
    "        return \"O\"\n",
    "\n",
    "    ## Iterator over the sentences in the dataset\n",
    "    def sentences(self):\n",
    "        for sid in self.data:\n",
    "            yield self.data[sid]\n",
    "\n",
    "    ## Iterator over the sentence IDs\n",
    "    def sentence_ids(self):\n",
    "        for sid in self.data:\n",
    "            yield sid\n",
    "\n",
    "    ## Get a sentence by its ID\n",
    "    def get_sentence(self, sid):\n",
    "        return self.data[sid]\n",
    "\n",
    "    ## Get sentences as lists\n",
    "    def tokens(self):\n",
    "        for sid in self.data:\n",
    "            s = []\n",
    "            for w in self.data[sid]:\n",
    "                s.append((sid, w['form'], w['start'], w['end']))\n",
    "            yield s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824eb3c3",
   "metadata": {},
   "source": [
    "# Codemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44f850a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Codemaps:\n",
    "\n",
    "    def __init__(self, data, maxlen=None, suflen=None):\n",
    "        \"\"\"\n",
    "        Default constructor\n",
    "        :param data:\n",
    "        :param maxlen:\n",
    "        :param suflen:\n",
    "        \"\"\"\n",
    "        if isinstance(data, Dataset) and maxlen is not None and suflen is not None:\n",
    "            self.__create_indexs(data, maxlen, suflen)\n",
    "\n",
    "        elif type(data) == str and maxlen is None and suflen is None:\n",
    "            self.__load(data)\n",
    "\n",
    "        else:\n",
    "            print('codemaps: Invalid or missing parameters in constructor')\n",
    "            exit()\n",
    "\n",
    "    def __create_indexs(self, data, maxlen, suflen):\n",
    "        \"\"\"\n",
    "        Create indices from the training corpus\n",
    "        Extracts all words and labels in the given sentences and creates indices to encode them as numbers\n",
    "        :param data:\n",
    "        :param maxlen:\n",
    "        :param suflen:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "        self.suflen = suflen\n",
    "        words = set([])\n",
    "        lc_words = set([])\n",
    "        sufs = set([])\n",
    "        labels = set([])\n",
    "\n",
    "        for s in data.sentences():\n",
    "            for t in s:\n",
    "                words.add(t['form'])\n",
    "                sufs.add(t['lc_form'][-self.suflen:])\n",
    "                labels.add(t['tag'])\n",
    "\n",
    "        self.word_index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "        self.word_index['PAD'] = 0  # Padding\n",
    "        self.word_index['UNK'] = 1  # Unknown words\n",
    "\n",
    "        self.suf_index = {s: i + 2 for i, s in enumerate(list(sufs))}\n",
    "        self.suf_index['PAD'] = 0  # Padding\n",
    "        self.suf_index['UNK'] = 1  # Unknown suffixes\n",
    "\n",
    "        self.label_index = {t: i + 1 for i, t in enumerate(list(labels))}\n",
    "        self.label_index['PAD'] = 0  # Padding\n",
    "\n",
    "    def __load(self, name):\n",
    "        \"\"\"\n",
    "        Load the indices\n",
    "        :param name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.maxlen = 0\n",
    "        self.suflen = 0\n",
    "        self.word_index = {}\n",
    "        self.suf_index = {}\n",
    "        self.label_index = {}\n",
    "\n",
    "        with open(name + \".idx\") as f:\n",
    "            for line in f.readlines():\n",
    "                (t, k, i) = line.split()\n",
    "                if t == 'MAXLEN':\n",
    "                    self.maxlen = int(k)\n",
    "                elif t == 'SUFLEN':\n",
    "                    self.suflen = int(k)\n",
    "                elif t == 'WORD':\n",
    "                    self.word_index[k] = int(i)\n",
    "                elif t == 'SUF':\n",
    "                    self.suf_index[k] = int(i)\n",
    "                elif t == 'LABEL':\n",
    "                    self.label_index[k] = int(i)\n",
    "\n",
    "    def save(self, name):\n",
    "        \"\"\"\n",
    "        Save the indices\n",
    "        :param name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with open(name + \".idx\", \"w\") as f:\n",
    "            print('MAXLEN', self.maxlen, \"-\", file=f)\n",
    "            print('SUFLEN', self.suflen, \"-\", file=f)\n",
    "            for key in self.label_index: print('LABEL', key, self.label_index[key], file=f)\n",
    "            for key in self.word_index: print('WORD', key, self.word_index[key], file=f)\n",
    "            for key in self.suf_index: print('SUF', key, self.suf_index[key], file=f)\n",
    "\n",
    "    def encode_words(self, data):\n",
    "        \"\"\"\n",
    "        Encodes X (text to indices)\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Encode the words and add padding\n",
    "        Xw = [[self.word_index[w['form']] if w['form'] in self.word_index else self.word_index['UNK'] for w in s] for s\n",
    "              in data.sentences()]\n",
    "        Xw = pad_sequences(maxlen=self.maxlen, sequences=Xw, padding=\"post\", value=self.word_index['PAD'])\n",
    "        # Encode the suffixes and add padding\n",
    "        Xs = [[self.suf_index[w['lc_form'][-self.suflen:]] if w['lc_form'][-self.suflen:] in self.suf_index else\n",
    "               self.suf_index['UNK'] for w in s] for s in data.sentences()]\n",
    "        Xs = pad_sequences(maxlen=self.maxlen, sequences=Xs, padding=\"post\", value=self.suf_index['PAD'])\n",
    "        # Return the sentences\n",
    "        return [Xw, Xs]\n",
    "\n",
    "    def encode_labels(self, data):\n",
    "        \"\"\"\n",
    "        Encodes labels and adds padding\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        Y = [[self.label_index[w['tag']] for w in s] for s in data.sentences()]\n",
    "        Y = pad_sequences(maxlen=self.maxlen, sequences=Y, padding=\"post\", value=self.label_index[\"PAD\"])\n",
    "        return np.array(Y)\n",
    "\n",
    "    def get_n_words(self):\n",
    "        \"\"\"\n",
    "        Returns the number of word indices\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return len(self.word_index)\n",
    "\n",
    "    def get_n_sufs(self):\n",
    "        \"\"\"\n",
    "        Returns the number of suffix indices\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return len(self.suf_index)\n",
    "\n",
    "    def get_n_labels(self):\n",
    "        \"\"\"\n",
    "        Returns the number of labels\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return len(self.label_index)\n",
    "\n",
    "    def word2idx(self, w):\n",
    "        \"\"\"\n",
    "        Returns the index of a word\n",
    "        :param w:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.word_index[w]\n",
    "\n",
    "    def suff2idx(self, s):\n",
    "        \"\"\"\n",
    "        Returns the index of a suffix\n",
    "        :param s:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.suf_index[s]\n",
    "\n",
    "    def label2idx(self, l):\n",
    "        \"\"\"\n",
    "        Returns the index of a label\n",
    "        :param l:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.label_index[l]\n",
    "\n",
    "    def idx2label(self, i):\n",
    "        \"\"\"\n",
    "        Returns the label given an index\n",
    "        :param i:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for l in self.label_index:\n",
    "            if self.label_index[l] == i:\n",
    "                return l\n",
    "        raise KeyError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbc5c0",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e44947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(codes):\n",
    "    \"\"\"\n",
    "    Function that builds the neural network.\n",
    "    :param codes:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global lstm_units, embeddingW_dim, embeddingS_dim\n",
    "    \n",
    "    # sizes\n",
    "    n_words = codes.get_n_words() # Number of words in the vocabulary\n",
    "    n_sufs = codes.get_n_sufs() # Number of suffixes in the vocabulary\n",
    "    n_labels = codes.get_n_labels() # Number of word classes\n",
    "    max_len = codes.maxlen # Maximum length of each sentence, for padding\n",
    "\n",
    "    inptW = Input(shape=(max_len,))  # Word embedding layer\n",
    "    embW = Embedding(input_dim=n_words, output_dim=embeddingW_dim,\n",
    "                     input_length=max_len, mask_zero=True)(inptW)\n",
    "\n",
    "    inptS = Input(shape=(max_len,))  # Suffix embedding layer\n",
    "    embS = Embedding(input_dim=n_sufs, output_dim=embeddingS_dim,\n",
    "                     input_length=max_len, mask_zero=True)(inptS)\n",
    "\n",
    "    dropW = Dropout(0.1)(embW)\n",
    "    dropS = Dropout(0.1)(embS)\n",
    "    drops = concatenate([dropW, dropS])\n",
    "\n",
    "    # biLSTM\n",
    "    bilstm = Bidirectional(LSTM(units=lstm_units, return_sequences=True,\n",
    "                                recurrent_dropout=0.1))(drops)\n",
    "    # Fully connected layer with softmax activation function to obtain the labels\n",
    "    out = TimeDistributed(Dense(n_labels, activation=\"softmax\"))(bilstm)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model([inptW, inptS], out)\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2e76e",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b55b8",
   "metadata": {},
   "source": [
    "In this case, we comment the code line by line and use it as a base case to analyze the effect of modifying different parameters. The Dataset function assigns a series of features to each word: the word in lowercase, the raw word, the position of the first and last character in the sentence, and the tag according to B-I-O schema.\n",
    "\n",
    "Then, a vocabulary is created, and each word and suffix is assigned the corresponding index. This way, the dataset is transformed into vectors, which are then used to train the model and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e28a7dee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nicolas.deza\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\nicolas.deza\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 150)]                0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 150)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 150, 50)              483800    ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 150, 50)              247850    ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 150, 50)              0         ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 150, 50)              0         ['embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 150, 100)             0         ['dropout[0][0]',             \n",
      "                                                                     'dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 150, 20)              8880      ['concatenate[0][0]']         \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 150, 10)              210       ['bidirectional[0][0]']       \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 740740 (2.83 MB)\n",
      "Trainable params: 740740 (2.83 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\nicolas.deza\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\nicolas.deza\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "170/170 [==============================] - 40s 199ms/step - loss: 0.8397 - accuracy: 0.8729 - val_loss: 0.4388 - val_accuracy: 0.8786\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 36s 214ms/step - loss: 0.3323 - accuracy: 0.8903 - val_loss: 0.3044 - val_accuracy: 0.9087\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 36s 209ms/step - loss: 0.2258 - accuracy: 0.9361 - val_loss: 0.2297 - val_accuracy: 0.9397\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 38s 222ms/step - loss: 0.1630 - accuracy: 0.9557 - val_loss: 0.1908 - val_accuracy: 0.9520\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 38s 223ms/step - loss: 0.1283 - accuracy: 0.9637 - val_loss: 0.1684 - val_accuracy: 0.9571\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 38s 223ms/step - loss: 0.1043 - accuracy: 0.9685 - val_loss: 0.1557 - val_accuracy: 0.9602\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 43s 253ms/step - loss: 0.0857 - accuracy: 0.9779 - val_loss: 0.1470 - val_accuracy: 0.9631\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 38s 223ms/step - loss: 0.0707 - accuracy: 0.9818 - val_loss: 0.1434 - val_accuracy: 0.9641\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 41s 240ms/step - loss: 0.0594 - accuracy: 0.9847 - val_loss: 0.1415 - val_accuracy: 0.9658\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 38s 226ms/step - loss: 0.0502 - accuracy: 0.9884 - val_loss: 0.1386 - val_accuracy: 0.9672\n",
      "INFO:tensorflow:Assets written to: models/model_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/model_1\\assets\n"
     ]
    }
   ],
   "source": [
    "# Directory with the files\n",
    "traindir = 'data/train'\n",
    "validationdir = 'data/devel'\n",
    "modelname = 'models/model_1'\n",
    "\n",
    "# Load the training and validation corpora\n",
    "# ******************************************************************\n",
    "# The following features are assigned to each word:\n",
    "# - lc_form: word in lowercase\n",
    "# - form: raw word\n",
    "# - start: position of the first character in the sentence\n",
    "# - end: position of the last character in the sentence\n",
    "# - tag: word classification, according to B-I-O\n",
    "# ******************************************************************\n",
    "traindata = Dataset(traindir)\n",
    "valdata = Dataset(validationdir)\n",
    "\n",
    "# Create the indices\n",
    "# ******************************************************************\n",
    "# When initializing the object of the Codemaps class, sets with index identifiers are created for each word, suffix, and class.\n",
    "# It is used to encode sentences\n",
    "# ******************************************************************\n",
    "max_len = 150 # Maximum length for padding\n",
    "suf_len = 5 # Last characters of each word to obtain suffix\n",
    "codes = Codemaps(traindata, max_len, suf_len)\n",
    "\n",
    "# Build the network\n",
    "# ******************************************************************\n",
    "# Bidirectional LSTM model, with Embedding and Dropout layer\n",
    "# ******************************************************************\n",
    "# These are some model parameters (they are imported in the build_network function as global variables)\n",
    "lstm_units = 10\n",
    "embeddingW_dim = 50\n",
    "embeddingS_dim = 50\n",
    "\n",
    "model = build_network(codes)\n",
    "model.summary()\n",
    "\n",
    "# Map the datasets\n",
    "# ******************************************************************\n",
    "# Encodes each sentence by assigning the corresponding index to each word or suffix, and applies padding\n",
    "# Also encodes the classifications\n",
    "# ******************************************************************\n",
    "Xt = codes.encode_words(traindata)\n",
    "Yt = codes.encode_labels(traindata)\n",
    "Xv = codes.encode_words(valdata)\n",
    "Yv = codes.encode_labels(valdata)\n",
    "\n",
    "# Train the model\n",
    "model.fit(Xt, Yt, batch_size=32, epochs=10, validation_data=(Xv, Yv), verbose=1)\n",
    "\n",
    "# Save the model and the indices\n",
    "model.save(modelname)\n",
    "codes.save(modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a02cb49",
   "metadata": {},
   "source": [
    "# Change dimensions for Embbeding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48802ad9",
   "metadata": {},
   "source": [
    "Results do not improve, lower dimensions seem to work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "187a819c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 43s 214ms/step - loss: 1.0012 - accuracy: 0.8736 - val_loss: 0.5454 - val_accuracy: 0.8781\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 37s 219ms/step - loss: 0.4557 - accuracy: 0.8834 - val_loss: 0.4087 - val_accuracy: 0.8790\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 37s 216ms/step - loss: 0.3141 - accuracy: 0.8941 - val_loss: 0.3073 - val_accuracy: 0.9104\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 36s 210ms/step - loss: 0.2402 - accuracy: 0.9312 - val_loss: 0.2662 - val_accuracy: 0.9275\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 33s 194ms/step - loss: 0.2075 - accuracy: 0.9374 - val_loss: 0.2453 - val_accuracy: 0.9297\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 35s 207ms/step - loss: 0.1832 - accuracy: 0.9403 - val_loss: 0.2208 - val_accuracy: 0.9352\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 38s 225ms/step - loss: 0.1520 - accuracy: 0.9555 - val_loss: 0.1949 - val_accuracy: 0.9552\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 34s 202ms/step - loss: 0.1249 - accuracy: 0.9665 - val_loss: 0.1746 - val_accuracy: 0.9595\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 34s 199ms/step - loss: 0.1054 - accuracy: 0.9693 - val_loss: 0.1614 - val_accuracy: 0.9614\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 34s 199ms/step - loss: 0.0909 - accuracy: 0.9730 - val_loss: 0.1578 - val_accuracy: 0.9630\n",
      "INFO:tensorflow:Assets written to: embedding_10_dim\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: embedding_10_dim\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 59s 305ms/step - loss: 0.6016 - accuracy: 0.8879 - val_loss: 0.2912 - val_accuracy: 0.9237\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 56s 329ms/step - loss: 0.2067 - accuracy: 0.9426 - val_loss: 0.2105 - val_accuracy: 0.9457\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 53s 311ms/step - loss: 0.1361 - accuracy: 0.9620 - val_loss: 0.1710 - val_accuracy: 0.9582\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 53s 314ms/step - loss: 0.0904 - accuracy: 0.9769 - val_loss: 0.1572 - val_accuracy: 0.9633\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 53s 314ms/step - loss: 0.0642 - accuracy: 0.9858 - val_loss: 0.1486 - val_accuracy: 0.9660\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 52s 307ms/step - loss: 0.0482 - accuracy: 0.9898 - val_loss: 0.1512 - val_accuracy: 0.9670\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 53s 310ms/step - loss: 0.0378 - accuracy: 0.9919 - val_loss: 0.1488 - val_accuracy: 0.9677\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 53s 311ms/step - loss: 0.0305 - accuracy: 0.9930 - val_loss: 0.1503 - val_accuracy: 0.9680\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 54s 319ms/step - loss: 0.0254 - accuracy: 0.9942 - val_loss: 0.1493 - val_accuracy: 0.9684\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 53s 311ms/step - loss: 0.0215 - accuracy: 0.9952 - val_loss: 0.1523 - val_accuracy: 0.9675\n",
      "INFO:tensorflow:Assets written to: embedding_300_dim\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: embedding_300_dim\\assets\n"
     ]
    }
   ],
   "source": [
    "dimensions = [10, 300]\n",
    "model_names = ['embedding_10_dim', 'embedding_300_dim']\n",
    "\n",
    "for dimension, model_name in zip(dimensions, model_names):\n",
    "    clear_session()\n",
    "    \n",
    "    embeddingW_dim, embeddingS_dim = dimension, dimension\n",
    "    model = build_network(codes)\n",
    "    \n",
    "    model.fit(Xt, Yt, batch_size=32, epochs=10, validation_data=(Xv, Yv), verbose=1)\n",
    "    model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccbeafe",
   "metadata": {},
   "source": [
    "# Change max Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eebba0",
   "metadata": {},
   "source": [
    "Result do not improve increasing sentence length. So we choose lower length to decrease computational cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72225b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 25s 93ms/step - loss: 1.0057 - accuracy: 0.8717 - val_loss: 0.5611 - val_accuracy: 0.8783\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 15s 88ms/step - loss: 0.4809 - accuracy: 0.8844 - val_loss: 0.4486 - val_accuracy: 0.8787\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 14s 81ms/step - loss: 0.3519 - accuracy: 0.8865 - val_loss: 0.3384 - val_accuracy: 0.8940\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 14s 85ms/step - loss: 0.2631 - accuracy: 0.9212 - val_loss: 0.2843 - val_accuracy: 0.9238\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 14s 85ms/step - loss: 0.2237 - accuracy: 0.9364 - val_loss: 0.2604 - val_accuracy: 0.9286\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 14s 82ms/step - loss: 0.2002 - accuracy: 0.9391 - val_loss: 0.2478 - val_accuracy: 0.9301\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 14s 82ms/step - loss: 0.1809 - accuracy: 0.9420 - val_loss: 0.2319 - val_accuracy: 0.9360\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 14s 83ms/step - loss: 0.1609 - accuracy: 0.9482 - val_loss: 0.2117 - val_accuracy: 0.9429\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 14s 84ms/step - loss: 0.1381 - accuracy: 0.9582 - val_loss: 0.1885 - val_accuracy: 0.9554\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 15s 89ms/step - loss: 0.1140 - accuracy: 0.9679 - val_loss: 0.1705 - val_accuracy: 0.9590\n",
      "INFO:tensorflow:Assets written to: max_len_50\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: max_len_50\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 87s 458ms/step - loss: 0.9869 - accuracy: 0.8720 - val_loss: 0.5608 - val_accuracy: 0.8781\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 71s 415ms/step - loss: 0.4790 - accuracy: 0.8835 - val_loss: 0.4366 - val_accuracy: 0.8786\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 72s 423ms/step - loss: 0.3409 - accuracy: 0.8901 - val_loss: 0.3217 - val_accuracy: 0.9078\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 64s 374ms/step - loss: 0.2539 - accuracy: 0.9295 - val_loss: 0.2733 - val_accuracy: 0.9284\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 70s 411ms/step - loss: 0.2182 - accuracy: 0.9374 - val_loss: 0.2533 - val_accuracy: 0.9300\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 70s 409ms/step - loss: 0.1975 - accuracy: 0.9390 - val_loss: 0.2376 - val_accuracy: 0.9316\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 61s 361ms/step - loss: 0.1771 - accuracy: 0.9419 - val_loss: 0.2202 - val_accuracy: 0.9377\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 62s 363ms/step - loss: 0.1564 - accuracy: 0.9517 - val_loss: 0.2029 - val_accuracy: 0.9489\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 62s 366ms/step - loss: 0.1366 - accuracy: 0.9612 - val_loss: 0.1861 - val_accuracy: 0.9561\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 61s 360ms/step - loss: 0.1191 - accuracy: 0.9669 - val_loss: 0.1738 - val_accuracy: 0.9599\n",
      "INFO:tensorflow:Assets written to: max_len_300\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: max_len_300\\assets\n"
     ]
    }
   ],
   "source": [
    "max_lens = [50, 300]\n",
    "model_names = ['max_len_50', 'max_len_300']\n",
    "\n",
    "for modelname, max_len in zip(model_names, max_lens):\n",
    "    clear_session()\n",
    "    \n",
    "    codes = Codemaps(traindata, max_len, 5)\n",
    "    \n",
    "    lstm_units = 10\n",
    "    embeddingW_dim, embeddingS_dim = 10, 10\n",
    "    model = build_network(codes)\n",
    "    \n",
    "    Xt = codes.encode_words(traindata)\n",
    "    Yt = codes.encode_labels(traindata)\n",
    "    Xv = codes.encode_words(valdata)\n",
    "    Yv = codes.encode_labels(valdata)\n",
    "    \n",
    "    model.fit(Xt, Yt, batch_size=32, epochs=10, validation_data=(Xv, Yv), verbose=1)\n",
    "    model.save(modelname)\n",
    "    codes.save(modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff275c87",
   "metadata": {},
   "source": [
    "# Change sufix length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ebd403",
   "metadata": {},
   "source": [
    "The suffix length that yields the best results for predicting the word category is 5 characters. Suffixes have variable lengths, and in each case, a different number of characters might be needed. However, the model cannot know this in advance, so a fixed length must be chosen. In this case, it is observed that on average, the length that works best is 5 characters, although the difference is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a8107ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 18s 70ms/step - loss: 1.0593 - accuracy: 0.8676 - val_loss: 0.5633 - val_accuracy: 0.8783\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 11s 65ms/step - loss: 0.4653 - accuracy: 0.8846 - val_loss: 0.4168 - val_accuracy: 0.8801\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 11s 65ms/step - loss: 0.3216 - accuracy: 0.8990 - val_loss: 0.3083 - val_accuracy: 0.9185\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 12s 69ms/step - loss: 0.2438 - accuracy: 0.9346 - val_loss: 0.2612 - val_accuracy: 0.9307\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 11s 67ms/step - loss: 0.2050 - accuracy: 0.9386 - val_loss: 0.2319 - val_accuracy: 0.9334\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 11s 66ms/step - loss: 0.1761 - accuracy: 0.9454 - val_loss: 0.2033 - val_accuracy: 0.9481\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 12s 72ms/step - loss: 0.1463 - accuracy: 0.9604 - val_loss: 0.1786 - val_accuracy: 0.9571\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 11s 65ms/step - loss: 0.1241 - accuracy: 0.9666 - val_loss: 0.1636 - val_accuracy: 0.9597\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 11s 65ms/step - loss: 0.1081 - accuracy: 0.9694 - val_loss: 0.1515 - val_accuracy: 0.9612\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 11s 67ms/step - loss: 0.0946 - accuracy: 0.9744 - val_loss: 0.1426 - val_accuracy: 0.9630\n",
      "INFO:tensorflow:Assets written to: suf_len_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: suf_len_3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 19s 73ms/step - loss: 1.0440 - accuracy: 0.8697 - val_loss: 0.5669 - val_accuracy: 0.8783\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 11s 66ms/step - loss: 0.4663 - accuracy: 0.8845 - val_loss: 0.4208 - val_accuracy: 0.8791\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 11s 67ms/step - loss: 0.3230 - accuracy: 0.8928 - val_loss: 0.3147 - val_accuracy: 0.9079\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 12s 69ms/step - loss: 0.2412 - accuracy: 0.9301 - val_loss: 0.2768 - val_accuracy: 0.9216\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 11s 66ms/step - loss: 0.2090 - accuracy: 0.9378 - val_loss: 0.2605 - val_accuracy: 0.9249\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 12s 69ms/step - loss: 0.1874 - accuracy: 0.9402 - val_loss: 0.2445 - val_accuracy: 0.9279\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 12s 69ms/step - loss: 0.1653 - accuracy: 0.9479 - val_loss: 0.2318 - val_accuracy: 0.9397\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 11s 66ms/step - loss: 0.1410 - accuracy: 0.9594 - val_loss: 0.2192 - val_accuracy: 0.9472\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 12s 68ms/step - loss: 0.1208 - accuracy: 0.9654 - val_loss: 0.2060 - val_accuracy: 0.9515\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 11s 66ms/step - loss: 0.1049 - accuracy: 0.9688 - val_loss: 0.1966 - val_accuracy: 0.9535\n",
      "INFO:tensorflow:Assets written to: suf_len_7\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: suf_len_7\\assets\n"
     ]
    }
   ],
   "source": [
    "suf_lens = [3, 7]\n",
    "model_names = ['suf_len_3', 'suf_len_7']\n",
    "\n",
    "\n",
    "for modelname, suf_len in zip(model_names, suf_lens):\n",
    "    clear_session()\n",
    "    \n",
    "    codes = Codemaps(traindata, 50, suf_len)\n",
    "    lstm_units = 10\n",
    "    embeddingW_dim, embeddingS_dim = 10, 10\n",
    "    model = build_network(codes)\n",
    "    \n",
    "    Xt = codes.encode_words(traindata)\n",
    "    Yt = codes.encode_labels(traindata)\n",
    "    Xv = codes.encode_words(valdata)\n",
    "    Yv = codes.encode_labels(valdata)\n",
    "    \n",
    "    model.fit(Xt, Yt, batch_size=32, epochs=10, validation_data=(Xv, Yv), verbose=1)\n",
    "    model.save(modelname)\n",
    "    codes.save(modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b0a63",
   "metadata": {},
   "source": [
    "# Increase LSTM units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71d937b",
   "metadata": {},
   "source": [
    "When we use 100 LSTM units in the model, the accuracy with the training data improves significantly. However, the accuracy with the validation data does not improve as much, and the difference between the two increases. This is due to overfitting, as expected when increasing the complexity of the model. We see that with 20 LSTM units, the result is similar to that with 100 LSTM units, and the overfitting is lower. Additionally, the computational cost of training the model is significantly lower with fewer LSTM units.\n",
    "\n",
    "When using 300 LSTM units, the accuracy with the validation data does not improve either, as expected.\n",
    "\n",
    "In conclusion, we see that it is better to use a limited number (20 is enough) of LSTM units, as this avoids overfitting, achieves similar results with the validation data, and the computational cost is lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9ecd97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 27s 126ms/step - loss: 0.6492 - accuracy: 0.8805 - val_loss: 0.3438 - val_accuracy: 0.9041\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 21s 126ms/step - loss: 0.2205 - accuracy: 0.9342 - val_loss: 0.2028 - val_accuracy: 0.9425\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 21s 122ms/step - loss: 0.1401 - accuracy: 0.9551 - val_loss: 0.1735 - val_accuracy: 0.9547\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 21s 126ms/step - loss: 0.1008 - accuracy: 0.9716 - val_loss: 0.1563 - val_accuracy: 0.9575\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 23s 138ms/step - loss: 0.0739 - accuracy: 0.9797 - val_loss: 0.1461 - val_accuracy: 0.9630\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 24s 144ms/step - loss: 0.0579 - accuracy: 0.9839 - val_loss: 0.1549 - val_accuracy: 0.9625\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 25s 145ms/step - loss: 0.0465 - accuracy: 0.9871 - val_loss: 0.1461 - val_accuracy: 0.9654\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 25s 144ms/step - loss: 0.0376 - accuracy: 0.9904 - val_loss: 0.1545 - val_accuracy: 0.9664\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 25s 145ms/step - loss: 0.0301 - accuracy: 0.9924 - val_loss: 0.1761 - val_accuracy: 0.9655\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 27s 161ms/step - loss: 0.0271 - accuracy: 0.9931 - val_loss: 0.1616 - val_accuracy: 0.9678\n",
      "INFO:tensorflow:Assets written to: more_lstm_units\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: more_lstm_units\\assets\n"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "\n",
    "codes = Codemaps(traindata, 50, 5)\n",
    "\n",
    "# 100 unidades\n",
    "lstm_units = 100\n",
    "embeddingW_dim, embeddingS_dim = 20, 20\n",
    "model = build_network(codes)\n",
    "    \n",
    "Xt = codes.encode_words(traindata)\n",
    "Yt = codes.encode_labels(traindata)\n",
    "Xv = codes.encode_words(valdata)\n",
    "Yv = codes.encode_labels(valdata)\n",
    "\n",
    "modelname = 'more_lstm_units'\n",
    "model.fit(Xt, Yt, batch_size=32, epochs=10, validation_data=(Xv, Yv), verbose=1)\n",
    "model.save(modelname)\n",
    "codes.save(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f71cb59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 37s 169ms/step - loss: 0.6440 - accuracy: 0.8795 - val_loss: 0.3499 - val_accuracy: 0.8958\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 25s 149ms/step - loss: 0.2242 - accuracy: 0.9322 - val_loss: 0.2152 - val_accuracy: 0.9398\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 27s 160ms/step - loss: 0.1444 - accuracy: 0.9539 - val_loss: 0.1901 - val_accuracy: 0.9522\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 27s 158ms/step - loss: 0.1004 - accuracy: 0.9709 - val_loss: 0.1692 - val_accuracy: 0.9597\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 27s 160ms/step - loss: 0.0752 - accuracy: 0.9791 - val_loss: 0.1596 - val_accuracy: 0.9620\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 28s 164ms/step - loss: 0.0595 - accuracy: 0.9836 - val_loss: 0.1628 - val_accuracy: 0.9619\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 27s 158ms/step - loss: 0.0480 - accuracy: 0.9868 - val_loss: 0.1587 - val_accuracy: 0.9640\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 26s 152ms/step - loss: 0.0392 - accuracy: 0.9894 - val_loss: 0.1705 - val_accuracy: 0.9619\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 26s 151ms/step - loss: 0.0308 - accuracy: 0.9921 - val_loss: 0.1672 - val_accuracy: 0.9649\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 30s 176ms/step - loss: 0.0257 - accuracy: 0.9937 - val_loss: 0.1744 - val_accuracy: 0.9639\n",
      "INFO:tensorflow:Assets written to: 300_lstm_units\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 300_lstm_units\\assets\n"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "\n",
    "codes = Codemaps(traindata, 50, 5)\n",
    "\n",
    "# 300 unidades\n",
    "lstm_units = 100\n",
    "embeddingW_dim, embeddingS_dim = 20, 20\n",
    "model = build_network(codes)\n",
    "    \n",
    "Xt = codes.encode_words(traindata)\n",
    "Yt = codes.encode_labels(traindata)\n",
    "Xv = codes.encode_words(valdata)\n",
    "Yv = codes.encode_labels(valdata)\n",
    "\n",
    "modelname = '300_lstm_units'\n",
    "model.fit(Xt, Yt, batch_size=32, epochs=10, validation_data=(Xv, Yv), verbose=1)\n",
    "model.save(modelname)\n",
    "codes.save(modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240c71e",
   "metadata": {},
   "source": [
    "# Repeat the exercise using prefix instead of sufix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ae9faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Codemaps_pref:\n",
    "\n",
    "    def __init__(self, data, maxlen=None, preflen=None):\n",
    "        \"\"\"\n",
    "        Default constructor\n",
    "        :param data:\n",
    "        :param maxlen:\n",
    "        :param preflen:\n",
    "        \"\"\"\n",
    "        if isinstance(data, Dataset) and maxlen is not None and preflen is not None:\n",
    "            self.__create_indexes(data, maxlen, preflen)\n",
    "\n",
    "        elif type(data) == str and maxlen is None and preflen is None:\n",
    "            self.__load(data)\n",
    "\n",
    "        else:\n",
    "            print('codemaps: Invalid or missing parameters in constructor')\n",
    "            exit()\n",
    "\n",
    "    def __create_indexes(self, data, maxlen, preflen):\n",
    "        \"\"\"\n",
    "        Create indexes from the learning corpus\n",
    "        Extracts all the words and labels in the given sentences and creates indexes to encode them as numbers\n",
    "        :param data:\n",
    "        :param maxlen:\n",
    "        :param preflen:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "        self.preflen = preflen\n",
    "        words = set([])\n",
    "        lc_words = set([])\n",
    "        prefs = set([])\n",
    "        labels = set([])\n",
    "\n",
    "        for s in data.sentences():\n",
    "            for t in s:\n",
    "                words.add(t['form'])\n",
    "                prefs.add(t['lc_form'][:self.preflen])\n",
    "                labels.add(t['tag'])\n",
    "\n",
    "        self.word_index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "        self.word_index['PAD'] = 0  # Padding\n",
    "        self.word_index['UNK'] = 1  # Unknown words\n",
    "\n",
    "        self.pref_index = {s: i + 2 for i, s in enumerate(list(prefs))}\n",
    "        self.pref_index['PAD'] = 0  # Padding\n",
    "        self.pref_index['UNK'] = 1  # Unknown prefixes\n",
    "\n",
    "        self.label_index = {t: i + 1 for i, t in enumerate(list(labels))}\n",
    "        self.label_index['PAD'] = 0  # Padding\n",
    "\n",
    "    def __load(self, name):\n",
    "        \"\"\"\n",
    "        Load the indexes\n",
    "        :param name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.maxlen = 0\n",
    "        self.preflen = 0\n",
    "        self.word_index = {}\n",
    "        self.pref_index = {}\n",
    "        self.label_index = {}\n",
    "\n",
    "        with open(name + \".idx\") as f:\n",
    "            for line in f.readlines():\n",
    "                (t, k, i) = line.split()\n",
    "                if t == 'MAXLEN':\n",
    "                    self.maxlen = int(k)\n",
    "                elif t == 'PREFLEN':\n",
    "                    self.preflen = int(k)\n",
    "                elif t == 'WORD':\n",
    "                    self.word_index[k] = int(i)\n",
    "                elif t == 'PREF':\n",
    "                    self.pref_index[k] = int(i)\n",
    "                elif t == 'LABEL':\n",
    "                    self.label_index[k] = int(i)\n",
    "\n",
    "    def save(self, name):\n",
    "        \"\"\"\n",
    "        Save the indexes\n",
    "        :param name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with open(name + \".idx\", \"w\") as f:\n",
    "            print('MAXLEN', self.maxlen, \"-\", file=f)\n",
    "            print('PREFLEN', self.preflen, \"-\", file=f)\n",
    "            for key in self.label_index: print('LABEL', key, self.label_index[key], file=f)\n",
    "            for key in self.word_index: print('WORD', key, self.word_index[key], file=f)\n",
    "            for key in self.pref_index: print('PREF', key, self.pref_index[key], file=f)\n",
    "\n",
    "    def encode_words(self, data):\n",
    "        \"\"\"\n",
    "        Encodes X (text to indexes)\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Encodes the words and adds padding\n",
    "        Xw = [[self.word_index[w['form']] if w['form'] in self.word_index else self.word_index['UNK'] for w in s] for s\n",
    "              in data.sentences()]\n",
    "        Xw = pad_sequences(maxlen=self.maxlen, sequences=Xw, padding=\"post\", value=self.word_index['PAD'])\n",
    "        # Encodes the prefixes and adds padding\n",
    "        Xs = [[self.pref_index[w['lc_form'][:self.preflen]] if w['lc_form'][:self.preflen] in self.pref_index else\n",
    "               self.pref_index['UNK'] for w in s] for s in data.sentences()]\n",
    "        Xs = pad_sequences(maxlen=self.maxlen, sequences=Xs, padding=\"post\", value=self.pref_index['PAD'])\n",
    "        # Returns the sentences\n",
    "        return [Xw, Xs]\n",
    "\n",
    "    def encode_labels(self, data):\n",
    "        \"\"\"\n",
    "        Encodes labels and adds padding\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        Y = [[self.label_index[w['tag']] for w in s] for s in data.sentences()]\n",
    "        Y = pad_sequences(maxlen=self.maxlen, sequences=Y, padding=\"post\", value=self.label_index[\"PAD\"])\n",
    "        return np.array(Y)\n",
    "\n",
    "    def get_n_words(self):\n",
    "        \"\"\"\n",
    "        Returns the number of word indexes\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return len(self.word_index)\n",
    "\n",
    "    def get_n_prefs(self):\n",
    "        \"\"\"\n",
    "        Returns the number of prefix indexes\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return len(self.pref_index)\n",
    "\n",
    "    def get_n_labels(self):\n",
    "        \"\"\"\n",
    "        Returns the number of labels\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return len(self.label_index)\n",
    "\n",
    "    def word2idx(self, w):\n",
    "        \"\"\"\n",
    "        Returns the index of a word\n",
    "        :param w:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.word_index[w]\n",
    "\n",
    "    def pref2idx(self, s):\n",
    "        \"\"\"\n",
    "        Returns the index of a prefix\n",
    "        :param s:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.pref_index[s]\n",
    "\n",
    "    def label2idx(self, l):\n",
    "        \"\"\"\n",
    "        Returns the index of a label\n",
    "        :param l:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.label_index[l]\n",
    "\n",
    "    def idx2label(self, i):\n",
    "        \"\"\"\n",
    "        Returns the label given an index\n",
    "        :param i:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for l in self.label_index:\n",
    "            if self.label_index[l] == i:\n",
    "                return l\n",
    "        raise KeyError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3da2485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network_pref(codes):\n",
    "    \"\"\"\n",
    "    Function that builds the neural network.\n",
    "    :param codes:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global lstm_units, embeddingW_dim, embeddingP_dim\n",
    "    \n",
    "    # sizes\n",
    "    n_words = codes.get_n_words() # Number of words in the vocabulary\n",
    "    n_prefs = codes.get_n_prefs() # Number of prefixes in the vocabulary\n",
    "    n_labels = codes.get_n_labels() # Number of word classes\n",
    "    max_len = codes.maxlen # Maximum length of each sentence, for padding\n",
    "\n",
    "    inptW = Input(shape=(max_len,))  # Word embedding layer\n",
    "    embW = Embedding(input_dim=n_words, output_dim=embeddingW_dim,\n",
    "                     input_length=max_len, mask_zero=True)(inptW)\n",
    "\n",
    "    inptP = Input(shape=(max_len,))  # Prefix embedding layer\n",
    "    embP = Embedding(input_dim=n_prefs, output_dim=embeddingP_dim,\n",
    "                     input_length=max_len, mask_zero=True)(inptP)\n",
    "\n",
    "    dropW = Dropout(0.1)(embW)\n",
    "    dropP = Dropout(0.1)(embP)\n",
    "    drops = concatenate([dropW, dropP])\n",
    "\n",
    "    # biLSTM\n",
    "    bilstm = Bidirectional(LSTM(units=lstm_units, return_sequences=True,\n",
    "                                recurrent_dropout=0.1))(drops)\n",
    "    # Fully connected layer with softmax activation function to obtain the labels\n",
    "    out = TimeDistributed(Dense(n_labels, activation=\"softmax\"))(bilstm)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model([inptW, inptP], out)\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6315429",
   "metadata": {},
   "source": [
    "The result is worse if we use prefixes. This makes sense, as the type or category of a word can be better predicted by looking at its ending (suffix). In fact, the accuracy with the training data improves, but not the accuracy with the validation data. This implies that there is greater overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81ae7941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 150)]                0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 150)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 150, 50)              483800    ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 150, 50)              258750    ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 150, 50)              0         ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 150, 50)              0         ['embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 150, 100)             0         ['dropout[0][0]',             \n",
      "                                                                     'dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 150, 40)              19360     ['concatenate[0][0]']         \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 150, 10)              410       ['bidirectional[0][0]']       \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 762320 (2.91 MB)\n",
      "Trainable params: 762320 (2.91 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 50s 244ms/step - loss: 0.7540 - accuracy: 0.8760 - val_loss: 0.4022 - val_accuracy: 0.8799\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 40s 233ms/step - loss: 0.2752 - accuracy: 0.9177 - val_loss: 0.2689 - val_accuracy: 0.9209\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 35s 203ms/step - loss: 0.1846 - accuracy: 0.9457 - val_loss: 0.2215 - val_accuracy: 0.9438\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 35s 205ms/step - loss: 0.1266 - accuracy: 0.9656 - val_loss: 0.1897 - val_accuracy: 0.9520\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 34s 202ms/step - loss: 0.0895 - accuracy: 0.9772 - val_loss: 0.1754 - val_accuracy: 0.9546\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 35s 205ms/step - loss: 0.0668 - accuracy: 0.9835 - val_loss: 0.1732 - val_accuracy: 0.9571\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 35s 208ms/step - loss: 0.0516 - accuracy: 0.9876 - val_loss: 0.1816 - val_accuracy: 0.9582\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 38s 225ms/step - loss: 0.0406 - accuracy: 0.9905 - val_loss: 0.1790 - val_accuracy: 0.9592\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 35s 206ms/step - loss: 0.0326 - accuracy: 0.9920 - val_loss: 0.1884 - val_accuracy: 0.9594\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 35s 205ms/step - loss: 0.0271 - accuracy: 0.9933 - val_loss: 0.1864 - val_accuracy: 0.9602\n",
      "INFO:tensorflow:Assets written to: models/model_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/model_1\\assets\n"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "\n",
    "# Directory with the files\n",
    "traindir = 'data/train'\n",
    "validationdir = 'data/devel'\n",
    "modelname = 'models/model_1'\n",
    "\n",
    "# Load the training and validation corpora\n",
    "# ******************************************************************\n",
    "# The following features are assigned to each word:\n",
    "# - lc_form: word in lowercase\n",
    "# - form: raw word\n",
    "# - start: position of the first character in the sentence\n",
    "# - end: position of the last character in the sentence\n",
    "# - tag: word classification, according to B-I-O\n",
    "# ******************************************************************\n",
    "traindata = Dataset(traindir)\n",
    "valdata = Dataset(validationdir)\n",
    "\n",
    "# Create the indexes\n",
    "# ******************************************************************\n",
    "# When initializing the object of the Codemaps class, sets with index identifiers are created for each word, suffix, and class.\n",
    "# It is used to encode sentences\n",
    "# ******************************************************************\n",
    "max_len = 150 # Maximum length for padding\n",
    "suf_len = 5 # Last characters of each word to obtain suffix\n",
    "codes = Codemaps_pref(traindata, max_len, suf_len)\n",
    "\n",
    "# Build the network\n",
    "# ******************************************************************\n",
    "# Bidirectional LSTM model, with Embedding and Dropout layer\n",
    "# ******************************************************************\n",
    "# These are some model parameters (imported into the build_network function as global variables)\n",
    "lstm_units = 20\n",
    "embeddingW_dim = 50\n",
    "embeddingP_dim = 50\n",
    "\n",
    "model = build_network_pref(codes)\n",
    "model.summary()\n",
    "\n",
    "# Map the datasets\n",
    "# ******************************************************************\n",
    "# Encodes each sentence by assigning the corresponding index to each word or suffix, and applies padding\n",
    "# Also encodes the classifications\n",
    "# ******************************************************************\n",
    "Xt = codes.encode_words(traindata)\n",
    "Yt = codes.encode_labels(traindata)\n",
    "Xv = codes.encode_words(valdata)\n",
    "Yv = codes.encode_labels(valdata)\n",
    "\n",
    "# Train the model\n",
    "model.fit(Xt, Yt, batch_size=32, epochs=10, validation_data=(Xv, Yv), verbose=1)\n",
    "\n",
    "# Save the model and the indexes\n",
    "model.save(modelname)\n",
    "codes.save(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbffa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79a0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3bb62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
